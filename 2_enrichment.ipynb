{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import ftfy\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from utils import search_engine, claim_extraction, QUOTES, emoji_pattern\n",
    "\n",
    "SPACES = re.compile(r\"\\s+\")\n",
    "LINES = re.compile(r\"(\\s*\\n\\s*)+\")\n",
    "BOLD = re.compile(r\"<b>(.*?)</b>\")\n",
    "STOPWORDS = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "PREPRO_DIR = \"data/parquet/prepro\"\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "\n",
    "for dataset_path in os.listdir(PREPRO_DIR):\n",
    "    data[dataset_path[:-8]] = pd.read_parquet(f'{PREPRO_DIR}/{dataset_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm:**\n",
    "\n",
    "1.  **Query Selection**\n",
    "    *   The entire claim if *n_words* <= 20 words\n",
    "    *   The exact first sentence if *n_words* >= 7\n",
    "    *   The first paragraph if *n_words* <= 20 words\n",
    "    *   The first 20 words\n",
    "2.  **Exact Search**\n",
    "    *   If there are exact *matches*, add an exact match flag and return *near matches*.\n",
    "    *   If not, perform *query_extraction* and search using the extracted query.\n",
    "        *   For *query_extraction*, extract up to 75 words.\n",
    "3.  **From the TOP 5 results, add a flag if it is an agency or a reliable source.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_with_search(text: str):\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    for quote in QUOTES:\n",
    "        text = text.replace(quote, \"\")\n",
    "    text = text.strip()\n",
    "\n",
    "    words = re.split(SPACES, text)\n",
    "\n",
    "    if len(words) <= 20:\n",
    "        query = text\n",
    "    else:\n",
    "        fst_sent = sent_tokenize(text, language='portuguese')[0]\n",
    "\n",
    "        if len(re.split(SPACES, fst_sent.strip())) >= 7:\n",
    "            query = fst_sent\n",
    "        else:\n",
    "            fst_paragraph = re.split(\"\\n+\", text)[0]\n",
    "            \n",
    "            if len(re.split(SPACES, fst_paragraph.strip())) < 20:\n",
    "                query = \" \".join(words[:20])\n",
    "            else:\n",
    "                query = fst_paragraph\n",
    "\n",
    "    results = search_engine(query, n=5)\n",
    "\n",
    "    query_no_punc = query.translate(str.maketrans('', '', string.punctuation))\n",
    "    words_set = set(re.split(SPACES, query_no_punc.strip().lower())) - STOPWORDS\n",
    "\n",
    "    had_match = False\n",
    "    if results:\n",
    "        for r in results:\n",
    "            if r[\"snippet\"]:\n",
    "                matches = re.findall(BOLD, r[\"snippet\"])\n",
    "                matches = [re.split(SPACES, w.strip().lower()) for w in matches if w.upper().isupper()]\n",
    "                \n",
    "                matches = {w for match in matches for w in match} - STOPWORDS\n",
    "                r[\"match\"] = len(words_set.intersection(matches))/len(words_set)\n",
    "            else:\n",
    "                r[\"match\"] = None\n",
    "\n",
    "            if r.get(\"match\") and r[\"match\"] > 0.8:\n",
    "                had_match = True\n",
    "\n",
    "    claim = None\n",
    "    if not had_match:\n",
    "        input_text  = \" \".join(words[:75]) if len(words) > 75 else text\n",
    "        claim = claim_extraction.invoke({\"text\": input_text})\n",
    "        claim = claim.content.strip()\n",
    "        for quote in QUOTES:\n",
    "            claim = claim.replace(quote, \"\")\n",
    "        results = search_engine(claim, n=5)\n",
    "        \n",
    "    results = pd.Series({\n",
    "        \"query\": query,\n",
    "        \"claim\": claim,\n",
    "        \"results\": results\n",
    "    })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"results.tmp\"):\n",
    "    with open(\"results.tmp\", \"w\") as f:\n",
    "        json.dump({}, f)\n",
    "\n",
    "with open(\"results.tmp\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "for dataset in ['MuMiN-PT', 'COVID19.BR', 'Fake.br']:\n",
    "    for idx, row in tqdm(data[dataset].iterrows(), total=data[dataset].shape[0]):\n",
    "        if f\"{dataset}_{idx}\" in results.keys() and results[f\"{dataset}_{idx}\"][\"results\"] != None:\n",
    "            continue\n",
    "        \n",
    "        result = extend_with_search(row[\"text_no_url\"])\n",
    "\n",
    "        if result[\"results\"] == None:\n",
    "            tqdm.write(row[\"text_no_url\"])\n",
    "\n",
    "        if \"query\" in row.index:\n",
    "            for column in result.columns:\n",
    "                row[column] = result[column]\n",
    "        else:\n",
    "            row = pd.concat([row, result])\n",
    "\n",
    "        results[f\"{dataset}_{idx}\"] = json.loads(row.to_json())\n",
    "        with open(\"results.tmp\", \"w\") as f:\n",
    "            json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['Fake.br', 'COVID19.BR', 'MuMiN-PT']:\n",
    "    data[dataset] = pd.DataFrame({\"_\".join(r.split(\"_\")[1:]): results[r] for r in results if dataset in r}).T\n",
    "    \n",
    "    data[dataset].to_parquet(f\"{PREPRO_DIR}/../final/{dataset}.parquet\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake_news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
