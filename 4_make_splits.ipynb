{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = \"../data/parquet/fact_check/\"\n",
    "\n",
    "SEED = 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = {\n",
    "    \"MuMiN-PT\": [\n",
    "        \"Associação Médica Americana\",\n",
    "        \"produtividade\",\n",
    "        \"leoa\",\n",
    "        \"Araraquara\",\n",
    "    ],\n",
    "    \"COVID19.BR\": [\n",
    "        \"Fernanda Torres\",\n",
    "        \"oms-pede-desculpas-pelo-erro-nas-controversia-sobre-hidroxicloroquina\",\n",
    "        \"assintomaticos-nao-contribuem-para-a-propagacao-do-virus\",\n",
    "        \"Segundo entendi\",\n",
    "        \"banco-mundial-classifica-o-brasil\",\n",
    "        \"vitamina D\",\n",
    "        \"Antiparasitário\",\n",
    "        \"45 - 19\",\n",
    "        \"FDA\",\n",
    "        \"nunca-aconselhou\",\n",
    "        \"brasilsemmedo.com/dra-nise-e-a-batalha-para-salvar-o-brasil-do-virus\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for dataset in corrections.keys():\n",
    "    fname = f'../data/parquet/fact_check/{dataset}.parquet'\n",
    "\n",
    "    df = pd.read_parquet(fname)\n",
    "    df[\"label\"] = df.apply(\n",
    "        lambda r: \"fake\" if any(span in r[\"text\"] for span in corrections[dataset]) else r[\"label\"], axis=True\n",
    "    )\n",
    "    df.to_parquet(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "raw_data = dict()\n",
    "near_duplicates = dict()\n",
    "fix = dict()\n",
    "\n",
    "for dataset_path in os.listdir(data_dir):\n",
    "    dataset = dataset_path[:-8]    \n",
    "    data[dataset] = pd.read_parquet(f\"{data_dir}/{dataset}.parquet\")\n",
    "    raw_data[dataset] = pd.read_parquet(f'../data/parquet/original/{dataset}.parquet')\n",
    "    raw_data[dataset].index = [str(idx) for idx in raw_data[dataset].index]\n",
    "\n",
    "    fix_path = f'../data/parquet/fix2/{dataset}.parquet'\n",
    "    if os.path.exists(fix_path):\n",
    "        fix[dataset] = pd.read_parquet(fix_path)\n",
    "    else:\n",
    "        fix[dataset] = None\n",
    "\n",
    "    near_duplicates[dataset] = pd.read_parquet(f'../data/parquet/prepro/{dataset}.parquet')[\"near_duplicates\"]\n",
    "    near_duplicates[dataset].index = [str(idx) for idx in near_duplicates[dataset].index]\n",
    "\n",
    "if \"fake_1\" in data[\"Fake.br\"].index:\n",
    "    index = [idx.split(\"_\") for idx in data[\"Fake.br\"].index]\n",
    "    data[\"Fake.br\"].index = [f\"{idx[0]}_{int(idx[1]):04}\" for idx in index]\n",
    "    data[\"Fake.br\"] = data[\"Fake.br\"].sort_index(key=lambda idx: idx.str.replace(r\"(.*)_(.*)\", r\"\\2_\\1\", regex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_labels(row):\n",
    "    if row.name not in data[dataset].index:\n",
    "        return False\n",
    "    \n",
    "    if \"manual_review\" in data[dataset].columns and data[dataset].loc[str(row.name)][\"manual_review\"] == True:\n",
    "        return False\n",
    "\n",
    "    return row[\"label\"] != data[dataset].loc[str(row.name)][\"label\"]\n",
    "\n",
    "def fix_few_tokens(row):\n",
    "    if row[\"only_url\"] | row[\"unrelated_content\"] | row[\"null\"]:\n",
    "        return False\n",
    "    \n",
    "    return row[\"few_tokens\"]\n",
    "\n",
    "for dataset in data:\n",
    "    review_idx = raw_data[dataset].columns.to_list().index(\"text_no_url\") + 1\n",
    "    raw_data[dataset][raw_data[dataset].columns[review_idx:]] = \\\n",
    "        raw_data[dataset][raw_data[dataset].columns[review_idx:]].fillna(False)\n",
    "    raw_data[dataset][\"google_fact_check\"] = raw_data[dataset].apply(lambda row: check_labels(row), axis=True)\n",
    "    if \"few_tokens\" in raw_data[dataset].columns:\n",
    "        raw_data[dataset][\"few_tokens\"] = raw_data[dataset].apply(fix_few_tokens, axis=True)\n",
    "    raw_data[dataset].to_parquet(f'../data/parquet/original/{dataset}.parquet')\n",
    "\n",
    "if data[\"Fake.br\"].value_counts(\"label\")[\"fake\"] == 3600:\n",
    "    to_remove = data[\"Fake.br\"].apply(lambda row: raw_data[\"Fake.br\"][\"label_align\"].get(row.name, True),axis=True)\n",
    "    data[\"Fake.br\"] = data[\"Fake.br\"][~to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_cols = {\n",
    "    \"query\": \"initial_query\",\n",
    "    \"claim\": \"claim_query\",\n",
    "    \"results\": \"google_search_results\",\n",
    "    'claim_results': \"google_fact_check_results\",\n",
    "    \"label\": \"label\"\n",
    "}\n",
    "\n",
    "for dataset in data:\n",
    "    if set(data[dataset].columns).intersection(rename_cols.keys()) != {\"label\"}:\n",
    "        derived_info = data[dataset][list(rename_cols.keys())].copy()\n",
    "        derived_info.rename(columns=rename_cols, inplace=True)\n",
    "        \n",
    "        remove_idx = raw_data[dataset].columns.to_list().index(\"text_no_url\") + 1\n",
    "        remove_cols = raw_data[dataset].columns[remove_idx:].to_list()\n",
    "\n",
    "        for relabel_col in [\"manual_review\", \"google_fact_check\"]:\n",
    "            if relabel_col in remove_cols:\n",
    "                remove_cols.remove(relabel_col)\n",
    "\n",
    "        not_removed = ~raw_data[dataset][remove_cols].any(axis=True)\n",
    "\n",
    "        new_data = raw_data[dataset][not_removed]\n",
    "\n",
    "        old_columns = new_data.columns[:remove_idx].to_list()\n",
    "        old_columns.remove(\"label\")\n",
    "\n",
    "        new_data = new_data[old_columns]\n",
    "        new_data.index = [str(idx) for idx in new_data.index]\n",
    "        \n",
    "        refactor_data = pd.concat([new_data, derived_info], axis=True)\n",
    "        refactor_data = refactor_data[refactor_data[\"text\"].notna()]\n",
    "        refactor_data[\"near_duplicates\"] = near_duplicates[dataset]\n",
    "\n",
    "        refactor_data[\"label\"] = refactor_data.apply(\n",
    "            lambda row: row.label if pd.notna(row.label) else raw_data[dataset].label[row.name], axis=True\n",
    "        )\n",
    "    else:\n",
    "        refactor_data = data[dataset]\n",
    "\n",
    "    if fix[dataset] is not None:\n",
    "\n",
    "        assert fix[dataset].index.equals(refactor_data[refactor_data[\"initial_query\"].isna()].index)\n",
    "\n",
    "        for new_column in rename_cols.values():\n",
    "            if new_column == \"label\":\n",
    "                continue\n",
    "\n",
    "            refactor_data[new_column] = refactor_data.apply(\n",
    "                lambda row: row[new_column] if not isinstance(row[new_column], float) \\\n",
    "                     else fix[dataset][new_column].get(row.name),\n",
    "                axis=True\n",
    "            )\n",
    "\n",
    "            \n",
    "\n",
    "    assert refactor_data.shape[0] == new_data.shape[0]\n",
    "    assert refactor_data[\"text\"].notna().all()\n",
    "    assert refactor_data[\"label\"].notna().all()\n",
    "\n",
    "    data[dataset] = refactor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COVID19.BR</th>\n",
       "      <th>Fake.br</th>\n",
       "      <th>MuMiN-PT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>only_url</th>\n",
       "      <td>501</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few_tokens</th>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unrelated_content</th>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_align</th>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google_fact_check</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contradicts</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url_duplicated</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not_pt</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_full_text</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manual_review</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duplicated</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>null</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   COVID19.BR  Fake.br  MuMiN-PT\n",
       "only_url                  501        0         0\n",
       "few_tokens                302        0         0\n",
       "unrelated_content          82        0         0\n",
       "label_align                 0       40         0\n",
       "google_fact_check          23        0         4\n",
       "contradicts                20        0         0\n",
       "url_duplicated              0       17         0\n",
       "not_pt                      8        0         0\n",
       "no_full_text                0        4         0\n",
       "manual_review               3        0         0\n",
       "duplicated                  0        1         0\n",
       "null                        1        0         0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = list()\n",
    "\n",
    "\n",
    "for dataset in data:\n",
    "    review_idx = raw_data[dataset].columns.to_list().index(\"text_no_url\") + 1\n",
    "    review_columns = raw_data[dataset].columns[review_idx:]\n",
    "    review = raw_data[dataset][review_columns]\n",
    "    review = review.sum().to_frame(dataset).T\n",
    "    reviews.append(review)\n",
    "\n",
    "reviews = pd.concat(reviews)\n",
    "reviews = reviews.fillna(0).astype(int)\n",
    "reviews = reviews.T\n",
    "reviews[\"total\"] = reviews.sum(axis=True)\n",
    "\n",
    "reviews = reviews.sort_values(\"total\", ascending=False)\n",
    "reviews.drop(columns=\"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      " & COVID19.BR & Fake.br & MuMiN-PT \\\\\n",
      "\\midrule\n",
      "\\textbf{unrelated\\_content} & 82 & 0 & 0 \\\\\n",
      "\\textbf{google\\_fact\\_check} & 23 & 0 & 4 \\\\\n",
      "\\textbf{contradicts} & 20 & 0 & 0 \\\\\n",
      "\\textbf{not\\_pt} & 8 & 0 & 0 \\\\\n",
      "\\textbf{manual\\_review} & 3 & 0 & 0 \\\\\n",
      "\\textbf{auto} & 804 & 1 & 0 \\\\\n",
      "\\textbf{Fake.br} & 0 & 61 & 0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_article = reviews.drop(columns=\"total\").copy()\n",
    "\n",
    "auto = [\"only_url\", \"few_tokens\", \"null\", \"duplicated\"]\n",
    "\n",
    "auto_df = review_article.loc[auto].sum().to_frame(\"auto\").T\n",
    "review_article = pd.concat([review_article, auto_df]).drop(index=auto)\n",
    "\n",
    "fakebr = [\"no_full_text\", \"url_duplicated\", \"label_align\"]\n",
    "\n",
    "fakebr_df = review_article.loc[fakebr].sum().to_frame(\"Fake.br\").T\n",
    "review_article = pd.concat([review_article, fakebr_df]).drop(index=fakebr)\n",
    "\n",
    "\n",
    "print(\n",
    "    review_article.to_latex(\n",
    "        bold_rows=True, escape=True, multicolumn_format=\"c\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake.br\n",
      "new_split\n",
      "train    0.8\n",
      "test     0.1\n",
      "dev      0.1\n",
      "Name: count, dtype: float64\n",
      "new_split\n",
      "train    0.798883\n",
      "test     0.100559\n",
      "dev      0.100559\n",
      "Name: count, dtype: float64\n",
      "------------\n",
      "COVID19.BR\n",
      "new_split\n",
      "train    0.799931\n",
      "test     0.100034\n",
      "dev      0.100034\n",
      "Name: count, dtype: float64\n",
      "new_split\n",
      "train    0.708103\n",
      "test     0.145949\n",
      "dev      0.145949\n",
      "Name: count, dtype: float64\n",
      "------------\n",
      "MuMiN-PT\n",
      "new_split\n",
      "train    0.799858\n",
      "test     0.100427\n",
      "dev      0.099715\n",
      "Name: count, dtype: float64\n",
      "new_split\n",
      "train    0.799858\n",
      "test     0.100427\n",
      "dev      0.099715\n",
      "Name: count, dtype: float64\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "show_columns = [ \n",
    "   \"text_no_url\", \"old_split\", \"new_split\", \"label\", \"initial_query\", \"claim_query\", \"google_search_results\", \"google_fact_check_results\", \"near_duplicates\", \"text_urls\", \"metadata\"\n",
    "]\n",
    "\n",
    "split_raw_data = dict()\n",
    "split_data = dict()\n",
    "for dataset in [\"Fake.br\", \"COVID19.BR\", \"MuMiN-PT\"]:\n",
    "   review_idx = raw_data[dataset].columns.to_list().index(\"text_no_url\") + 1\n",
    "   review_columns = raw_data[dataset].columns[review_idx:].to_list()\n",
    "\n",
    "   changed = raw_data[dataset][review_columns].any(axis=True)\n",
    "    \n",
    "   split_raw_data[dataset] = raw_data[dataset][[\"text_no_url\", \"label\"]].copy()\n",
    "   split_raw_data[dataset][\"old_split\"] = raw_data[dataset].get(\"split\")\n",
    "\n",
    "   split_raw_data[dataset][\"new_split\"] = split_raw_data[dataset].apply(\n",
    "      lambda r: \"train\" if changed.loc[r.name] else None,axis=True)\n",
    "\n",
    "   new_percent = 0.8*split_raw_data[dataset].shape[0] - changed.sum()\n",
    "   new_percent /= split_raw_data[dataset].shape[0] - changed.sum()  \n",
    "\n",
    "\n",
    "   if dataset == \"Fake.br\":\n",
    "      not_changed = {idx.split(\"_\")[1] for idx in split_raw_data[dataset][~changed].index}\n",
    "      _, test_dev_pair = train_test_split(list(not_changed), train_size=new_percent)\n",
    "\n",
    "      dev_pair, test_pair = train_test_split(test_dev_pair, test_size=0.5)\n",
    "      not_changed_idx = split_raw_data[dataset][~changed].apply(lambda r: r.name.split(\"_\")[-1], axis=True)\n",
    "\n",
    "      split_raw_data[dataset].loc[~changed, \"new_split\"] = [\n",
    "         \"test\" if item in test_pair else \"dev\" if item in dev_pair else \"train\" for item in not_changed_idx\n",
    "      ]\n",
    "   else:\n",
    "      not_changed_range = range(raw_data[dataset][~changed].shape[0])\n",
    "\n",
    "      _, test_dev = train_test_split(\n",
    "         not_changed_range, train_size=new_percent, stratify=split_raw_data[dataset][~changed][\"label\"]\n",
    "      )\n",
    "      dev, test = train_test_split(\n",
    "         test_dev, test_size=0.5, stratify=split_raw_data[dataset][~changed][\"label\"].iloc[test_dev]\n",
    "      )\n",
    "      \n",
    "      split_raw_data[dataset].loc[~changed, \"new_split\"] = [\n",
    "         \"test\" if item in test else \"dev\" if item in dev else \"train\" for item in not_changed_range\n",
    "      ]\n",
    "\n",
    "   split_info = split_raw_data[dataset].drop(columns=[\"label\"])\n",
    "   split_info.index = split_info.index.map(str)\n",
    "\n",
    "   split_data[dataset] = data[dataset].drop(columns=[\"text\", \"text_no_url\"])\n",
    "\n",
    "\n",
    "   split_data[dataset] = split_data[dataset].join(split_info)\n",
    "   split_data[dataset] = split_data[dataset][show_columns]\n",
    "   \n",
    "   \n",
    "   print(dataset)\n",
    "   print(split_raw_data[dataset][\"new_split\"].value_counts()/split_raw_data[dataset].shape[0])\n",
    "   print(split_data[dataset][\"new_split\"].value_counts()/split_data[dataset].shape[0])\n",
    "   print(\"-\"*12)\n",
    "\n",
    "   split_raw_data[dataset][\"text_no_url\"]\n",
    "\n",
    "   split_raw_data[dataset].to_parquet(f\"{data_dir}/../split/{dataset}_raw.parquet\")\n",
    "   split_data[dataset].to_parquet(f\"{data_dir}/../split/{dataset}.parquet\")\n",
    "\n",
    "\n",
    "   #assert split_raw_data[dataset][\"text_no_url\"].notna().all()\n",
    "   assert split_raw_data[dataset][\"label\"].notna().all()\n",
    "   #assert split_data[dataset][\"text_no_url\"].notna().all()\n",
    "   assert split_data[dataset][\"label\"].notna().all()\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake_news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
